{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate accelerate -q #Installs everything we might need\n",
    "from google.colab import drive\n",
    "print(\"Starting...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Drive mounted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "CSV_PATH = \"/content/drive/MyDrive/CS Projects/Winter Project 2526/Data/raw/bbc_data.csv\" #Sets the path to the CSV file\n",
    "\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH) #Checks and loads the CSV file\n",
    "    df = df.rename(columns={'data': 'text', 'labels': 'label_name'}) #Renames the columns\n",
    "\n",
    "    print(f\"Total documents loaded: {len(df)}\")\n",
    "    print(\"\\nFirst 5 rows (Check names!):\")\n",
    "    print(df.head())\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['label_name'].value_counts())\n",
    "else:\n",
    "    print(f\"ERROR: CSV file not found at: {CSV_PATH}\")\n",
    "    print(\"Please check your Google Drive path and file name.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d181c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #Loads the tokenizer\n",
    "\n",
    "if 'df' in globals() and not df.empty: #Tests on the first sample text from the dataframe\n",
    "    sample_text = df['text'].iloc[0]\n",
    "\n",
    "    tokenized_output = tokenizer(sample_text,\n",
    "                                 padding=True,\n",
    "                                 truncation=True)\n",
    "\n",
    "    print(\"\\n--- Tokenizer Check Successful ---\")\n",
    "    print(\"Sample Text:\", sample_text)\n",
    "    print(\"Input IDs:\", tokenized_output['input_ids'][:10])\n",
    "else:\n",
    "    print(\"DataFrame 'df' not found or is empty. Cannot test tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = df['label_name'].unique().tolist() #Shows every text category\n",
    "print(f\"Original Unique Labels: {unique_labels}\")\n",
    "\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)} #Creates a mapping between labels and integers\n",
    "id_to_label = {i: label for i, label in enumerate(unique_labels)} #Creates a reverse mapping between integers and labels\n",
    "\n",
    "print(f\"Created Label-to-ID Mapping: {label_map}\")\n",
    "\n",
    "df['label'] = df['label_name'].map(label_map) #Applies the mapping to the dataframe\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nVerification of New Labels:\")\n",
    "print(df[['label_name', 'label']].head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(df, preserve_index=False) #Converts Pandas dataframe to Hugging Face Dataset\n",
    "\n",
    "\n",
    "train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42) #Splits the dataset. 80% for training, 20% for testing/validation\n",
    "\n",
    "test_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42) #Splits the 20% set into 10% validation and 10% test\n",
    "\n",
    "dataset_dict = DatasetDict({ #Combines the splits into a DatasetDict object\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': test_valid_split['train'],\n",
    "    'test': test_valid_split['test']\n",
    "})\n",
    "\n",
    "print(\"\\n--- Dataset Split Information ---\")\n",
    "print(dataset_dict)\n",
    "print(f\"Train samples: {len(dataset_dict['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset_dict['validation'])}\")\n",
    "print(f\"Test samples: {len(dataset_dict['test'])}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #Loads the tokenizer\n",
    "\n",
    "def tokenize_function(examples): #Function to apply tokenization to several examples\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True) #Applies the function to the entire DatasetDict\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text', 'label_name']) #Removes the original, now unnecessary columns\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\") #Renames the label column to 'labels'\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\") #Sets the format to PyTorch tensors\n",
    "\n",
    "print(\"\\n--- Final Tokenized Dataset (Ready for Training) ---\")\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf61cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred): #Defines evaluation metrics\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    preds = np.argmax(predictions, axis=-1) #Converts logits to predicted class IDs\n",
    "\n",
    "    f1 = f1_score(labels, preds, average='weighted') #Calculates the weighted F1 score, as well as the accuracy\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_weighted': f1,\n",
    "    }\n",
    "\n",
    "num_labels = 5 #Initializes the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments( #Training configuration\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    #Memory optimization\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "\n",
    "    dataloader_num_workers=4, #Uses 4 CPU cores to prepare the next batch in parallel, making it faster\n",
    "\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    report_to='none',\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_weighted',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True #Enables faster training on the T4 GPU\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer( #Trainer Initialization\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting Fine-Tuning on T4 GPU ---\")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "output_save_path = \"/content/drive/MyDrive/CS Projects/Winter Project 2526/Models/Bert Model\" #Saves the final best model\n",
    "trainer.save_model(output_save_path)\n",
    "print(f\"\\n--- Training Complete! Best model saved to: {output_save_path} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e4f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
